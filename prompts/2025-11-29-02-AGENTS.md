Got it — Parlanchina, not Parlantina.

Given my current limits I can’t pull the repo directly, so I’ll consolidate everything I *already* know about Parlanchina and treat this as working context going forward.

Here’s the picture so far:

* **What Parlanchina is**

  * Local, simple **Python + Flask** web app.
  * Provides a **ChatGPT-style interface** you can run locally as a “plan B” when people can’t use cloud tools, Patina (Rust), etc.
  * Supports **chat history** and **basic memory per session**.
  * Already supports **Markdown** and **Mermaid** in responses.
  * Intended to support **MCP** (Model Context Protocol) soon / partially already.

* **UI layout (from your screenshot and earlier description)**

  * Left column: **session list / chat history** with timestamps.
  * Center: chat conversation view (messages from model and user).
  * Bottom center: **message input box**.
  * Below or near the input: UI for **MCP tool selection and invocation**.
  * Currently there is a **duplicate tool selector**:

    * One selector labeled “MCP TOOL” with ARGS JSON + “Run tool”.
    * Another selector of MCP tools just above the chat text box.

* **Current MCP behavior**

  * Parlanchina can connect to one or more MCP servers (e.g. `connect_db (postgres)`).
  * Frontend shows a dropdown to select a single tool and manually pass JSON arguments to “Run tool”.
  * There is **no automatic integration** of selected tools into the LLM call:

    * The LLM doesn’t receive the set of “enabled tools” as a tool list.
    * The user has to manually pick a tool and execute it.

* **Target behavior for MCP (your latest requirement)**

  * **Only one tools control** in the main chat UI.
  * That control should:

    * List **all available MCP tools**.
    * Use **checkboxes** to enable/disable each tool.
    * No manual “Run tool” per tool in the normal chat flow.
  * Backend should:

    * Maintain a **dictionary/registry of tools** (per server + name).
    * On each user message, take the list of **enabled tools** and pass them as the `tools` / functions list to the LLM.
    * Let the **LLM decide which of those tools to call** using tool-calling.
  * The UI’s responsibility is only to enable/disable tools, not to perform explicit calls.

* **Role of Parlanchina in your ecosystem**

  * It is the **Python analogue of Patina**:

    * Same basic idea (local, extensible AI chat client).
    * But easier to hack on / contribute to because it is Python and Flask, not Rust + egui.
  * Intended to be used in internal workshops and demos as a light, local **MCP-capable chat client**.

I will treat this as my current internal spec for **Parlanchina** and use it in future prompts (architecture, refactors, prompts for code-gen agents, etc.).

If you later want me to have a more precise model of the codebase itself, the fastest way is to paste or upload key files (e.g. `app.py`, main template, MCP client module) and I’ll integrate that into the working context.
